{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NVIDIA AI Data Platform: Enterprise RAG with Multimodal PDF Extraction\n",
        "\n",
        "**The Problem:** Most RAG demos work great on clean text but *fail* when they hit real enterprise documents â€” PDFs packed with data tables, charts, and mixed layouts. Ask a standard RAG system \"What was Q3 revenue?\" and you'll get a hallucinated guess, because the table got flattened into meaningless text.\n",
        "\n",
        "**The Solution:** [NVIDIA NVIngest](https://github.com/NVIDIA-AI-Blueprints/multimodal-pdf-data-extraction) (part of the NVIDIA AI Blueprint for RAG) extracts tables and charts as **separate, structured objects** â€” not just raw text. This means the LLM can actually read the table rows and give you the right number.\n",
        "\n",
        "In this notebook we will:\n",
        "1. **Ingest** a financial report PDF via the NVIngest API\n",
        "2. **Inspect** the structured JSON output â€” proving tables and charts are treated differently from text\n",
        "3. **Index** the results using LlamaIndex with NVIDIA NIMs for embeddings\n",
        "4. **Chat** with the data and get a precise, table-grounded answer\n",
        "\n",
        "---\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before running this notebook, make sure you have:\n",
        "- Run `bash setup.sh` to start the NVIngest and Milvus Docker services\n",
        "- Run `python generate_sample_pdf.py` to create the sample financial report\n",
        "- Installed Python dependencies: `pip install -r requirements.txt`\n",
        "- Set your `NGC_API_KEY` environment variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0 â€” Configuration & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# â”€â”€ Service endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "NVINGEST_URL = os.getenv(\"NVINGEST_URL\", \"http://localhost:8082\")\n",
        "MILVUS_URI   = os.getenv(\"MILVUS_URI\",   \"http://localhost:19530\")\n",
        "\n",
        "# â”€â”€ NVIDIA API Key (NGC_API_KEY or NVIDIA_API_KEY) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "NVIDIA_API_KEY = os.getenv(\"NGC_API_KEY\") or os.getenv(\"NVIDIA_API_KEY\", \"\")\n",
        "assert NVIDIA_API_KEY, (\n",
        "    \"Set NGC_API_KEY or NVIDIA_API_KEY in your environment.  \"\n",
        "    \"Get one at https://build.nvidia.com/\"\n",
        ")\n",
        "\n",
        "# â”€â”€ Path to the sample PDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PDF_PATH = Path(\"../data/sample_financial_report.pdf\")\n",
        "assert PDF_PATH.exists(), f\"PDF not found at {PDF_PATH}. Run: python generate_sample_pdf.py\"\n",
        "\n",
        "print(f\"NVIngest endpoint : {NVINGEST_URL}\")\n",
        "print(f\"Milvus endpoint   : {MILVUS_URI}\")\n",
        "print(f\"NVIDIA API Key    : {NVIDIA_API_KEY[:12]}...\")\n",
        "print(f\"Sample PDF        : {PDF_PATH}  ({PDF_PATH.stat().st_size / 1024:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick health check on NVIngest\n",
        "try:\n",
        "    resp = requests.get(f\"{NVINGEST_URL}/v1/health\", timeout=10)\n",
        "    print(f\"NVIngest health: {resp.status_code} â€” {resp.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Could not reach NVIngest at {NVINGEST_URL}: {e}\")\n",
        "    print(\"   Make sure you ran:  bash setup.sh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1 â€” Ingest: Send the PDF to NVIngest\n",
        "\n",
        "NVIngest provides a REST API that accepts PDF documents and returns structured\n",
        "extractions.  Unlike a simple text parser (PyPDF2, pdfplumber), NVIngest uses\n",
        "GPU-accelerated models to:\n",
        "\n",
        "- **Detect tables** via YOLOX object detection\n",
        "- **Parse table structure** via DePlot / table-transformer\n",
        "- **OCR** any scanned content via PaddleOCR\n",
        "- **Caption charts** via a Vision-Language Model\n",
        "\n",
        "The result: each element is tagged with its *type* (`text`, `table`, `image`)\n",
        "so downstream systems can treat them appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Approach A: Using the nv-ingest-client library â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#\n",
        "# The official Python client wraps the REST API with a convenient job-based\n",
        "# interface.  If you have `nv-ingest-client` installed, this is the\n",
        "# recommended path.\n",
        "\n",
        "USE_CLIENT_LIB = True  # Set to False to use the raw REST fallback below\n",
        "\n",
        "nvingest_results = None\n",
        "\n",
        "if USE_CLIENT_LIB:\n",
        "    try:\n",
        "        from nv_ingest_client.client import NvIngestClient\n",
        "        from nv_ingest_client.message_clients.rest.rest_client import RESTClient\n",
        "        from nv_ingest_client.primitives.tasks import (\n",
        "            ExtractTask,\n",
        "            SplitTask,\n",
        "        )\n",
        "        from nv_ingest_client.primitives.jobs import JobSpec\n",
        "        from nv_ingest_client.util.file_processing.extract import extract_file_content\n",
        "\n",
        "        print(\"âœ… nv-ingest-client library found.\")\n",
        "\n",
        "        # Create the client pointing at our local NVIngest server\n",
        "        client = NvIngestClient(\n",
        "            message_client_allocator=RESTClient,\n",
        "            message_client_hostname=\"localhost\",\n",
        "            message_client_port=8082,\n",
        "        )\n",
        "\n",
        "        # Read and encode the PDF\n",
        "        file_content, file_type = extract_file_content(str(PDF_PATH))\n",
        "\n",
        "        # Build a job specification\n",
        "        job_spec = JobSpec(\n",
        "            payload=file_content,\n",
        "            source_id=str(PDF_PATH),\n",
        "            source_name=PDF_PATH.name,\n",
        "            document_type=file_type,\n",
        "        )\n",
        "\n",
        "        # Add extraction task â€” extract text, tables, and charts\n",
        "        extract_task = ExtractTask(\n",
        "            document_type=file_type,\n",
        "            extract_text=True,\n",
        "            extract_images=True,\n",
        "            extract_tables=True,\n",
        "        )\n",
        "        job_spec.add_task(extract_task)\n",
        "\n",
        "        # Optional: add a splitting/chunking task\n",
        "        split_task = SplitTask(\n",
        "            split_by=\"sentence\",\n",
        "            split_length=10,\n",
        "            split_overlap=0,\n",
        "        )\n",
        "        job_spec.add_task(split_task)\n",
        "\n",
        "        # Submit the job\n",
        "        job_id = client.add_job(job_spec)\n",
        "        client.submit_job(job_id, job_queue_id=\"morpheus_task_queue\")\n",
        "\n",
        "        # Wait for results\n",
        "        result = client.fetch_job_result(job_id, timeout=120)\n",
        "        nvingest_results = result\n",
        "\n",
        "        print(f\"âœ… Ingestion complete. Received {len(result)} result set(s).\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸  nv-ingest-client not installed. Falling back to REST API...\")\n",
        "        USE_CLIENT_LIB = False\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Client library call failed: {e}\")\n",
        "        print(\"   Falling back to REST API approach...\")\n",
        "        USE_CLIENT_LIB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Approach B: Raw REST API fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#\n",
        "# If the nv-ingest-client is not available or the API has changed, we can\n",
        "# always fall back to the HTTP REST endpoint.\n",
        "\n",
        "if not USE_CLIENT_LIB or nvingest_results is None:\n",
        "    print(\"Using REST API to ingest the PDF...\")\n",
        "\n",
        "    # Read and base64-encode the PDF\n",
        "    pdf_bytes = PDF_PATH.read_bytes()\n",
        "    pdf_b64 = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
        "\n",
        "    # Build the ingestion payload\n",
        "    payload = {\n",
        "        \"documents\": [\n",
        "            {\n",
        "                \"content\": pdf_b64,\n",
        "                \"document_type\": \"pdf\",\n",
        "                \"source_id\": str(PDF_PATH),\n",
        "                \"source_name\": PDF_PATH.name,\n",
        "            }\n",
        "        ],\n",
        "        \"extraction_config\": {\n",
        "            \"extract_text\": True,\n",
        "            \"extract_tables\": True,\n",
        "            \"extract_images\": True,\n",
        "            \"extract_charts\": True,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    resp = requests.post(\n",
        "        f\"{NVINGEST_URL}/v1/ingest\",\n",
        "        json=payload,\n",
        "        headers=headers,\n",
        "        timeout=300,\n",
        "    )\n",
        "    resp.raise_for_status()\n",
        "    nvingest_results = resp.json()\n",
        "\n",
        "    print(f\"âœ… Ingestion complete (REST). Status: {resp.status_code}\")\n",
        "    print(f\"   Response keys: {list(nvingest_results.keys()) if isinstance(nvingest_results, dict) else type(nvingest_results).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 â€” Inspect: Explore the Structured Extraction Output\n",
        "\n",
        "This is the key insight: NVIngest does **not** return a flat wall of text.\n",
        "Instead, each element is classified by `type`:\n",
        "\n",
        "| Type    | What it means                                    |\n",
        "|---------|--------------------------------------------------|\n",
        "| `text`  | Narrative paragraphs, headings                   |\n",
        "| `table` | A detected table with row/column structure        |\n",
        "| `image` | A chart, figure, or infographic                  |\n",
        "\n",
        "Let's look at what came back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Parse and normalise the results into a flat list of chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#\n",
        "# The exact shape of `nvingest_results` depends on the NVIngest version.\n",
        "# We normalise it into a list of dicts, each with at least:\n",
        "#   { \"type\": \"text\"|\"table\"|\"image\", \"content\": ..., \"metadata\": {...} }\n",
        "\n",
        "def flatten_results(raw):\n",
        "    \"\"\"Extract a flat list of content chunks from NVIngest output.\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Handle different response shapes\n",
        "    if isinstance(raw, list):\n",
        "        items = raw\n",
        "    elif isinstance(raw, dict):\n",
        "        # Might be wrapped in {\"data\": [...]} or {\"documents\": [...]}\n",
        "        items = raw.get(\"data\", raw.get(\"documents\", raw.get(\"results\", [raw])))\n",
        "    else:\n",
        "        items = [raw]\n",
        "\n",
        "    for item in items:\n",
        "        if isinstance(item, list):\n",
        "            # Nested list â€” recurse\n",
        "            chunks.extend(flatten_results(item))\n",
        "        elif isinstance(item, dict):\n",
        "            # Check for standard keys\n",
        "            content_type = (\n",
        "                item.get(\"document_type\")\n",
        "                or item.get(\"type\")\n",
        "                or item.get(\"metadata\", {}).get(\"content_metadata\", {}).get(\"type\")\n",
        "                or \"text\"\n",
        "            )\n",
        "            content = (\n",
        "                item.get(\"content\")\n",
        "                or item.get(\"text\")\n",
        "                or item.get(\"data\")\n",
        "                or item.get(\"metadata\", {}).get(\"content\", \"\")\n",
        "            )\n",
        "            # Map \"structured\" to \"table\" for clarity\n",
        "            if content_type in (\"structured\", \"table_data\"):\n",
        "                content_type = \"table\"\n",
        "            if content_type in (\"chart\", \"figure\", \"infographic\"):\n",
        "                content_type = \"image\"\n",
        "\n",
        "            chunks.append({\n",
        "                \"type\": content_type,\n",
        "                \"content\": content if isinstance(content, str) else json.dumps(content, indent=2) if content else \"\",\n",
        "                \"metadata\": item.get(\"metadata\", {}),\n",
        "            })\n",
        "        elif isinstance(item, str):\n",
        "            chunks.append({\"type\": \"text\", \"content\": item, \"metadata\": {}})\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "chunks = flatten_results(nvingest_results)\n",
        "print(f\"Total chunks extracted: {len(chunks)}\")\n",
        "print()\n",
        "\n",
        "# Summary by type\n",
        "type_counts = Counter(c[\"type\"] for c in chunks)\n",
        "print(\"Chunk types:\")\n",
        "for t, count in type_counts.most_common():\n",
        "    print(f\"  {t:10s}  â†’  {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Show ALL chunks (truncated for readability) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"=\" * 80)\n",
        "print(\"FULL EXTRACTION OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    content_preview = chunk[\"content\"][:300] if chunk[\"content\"] else \"(empty)\"\n",
        "    print(f\"\\nâ”€â”€â”€ Chunk {i}  â”‚  type: {chunk['type']}  â”€â”€â”€\")\n",
        "    print(content_preview)\n",
        "    if len(chunk[\"content\"]) > 300:\n",
        "        print(f\"  ... ({len(chunk['content'])} chars total)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Highlight TABLE chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "table_chunks = [c for c in chunks if c[\"type\"] == \"table\"]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ğŸ” TABLE CHUNKS FOUND: {len(table_chunks)}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if table_chunks:\n",
        "    for i, tc in enumerate(table_chunks):\n",
        "        print(f\"\\nâ”€â”€ Table {i + 1} â”€â”€\")\n",
        "        print(tc[\"content\"][:1000])\n",
        "        print()\n",
        "else:\n",
        "    print(\"\\n(No table chunks detected. Check NVIngest configuration.)\")\n",
        "    print(\"Tip: Tables might be classified under a different type key.\")\n",
        "    print(\"Let's check all unique types:\", set(c['type'] for c in chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Highlight IMAGE / CHART chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "image_chunks = [c for c in chunks if c[\"type\"] == \"image\"]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ğŸ–¼ï¸  IMAGE / CHART CHUNKS FOUND: {len(image_chunks)}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if image_chunks:\n",
        "    for i, ic in enumerate(image_chunks):\n",
        "        print(f\"\\nâ”€â”€ Image {i + 1} â”€â”€\")\n",
        "        # Images may be base64-encoded; show metadata instead of raw data\n",
        "        meta = ic.get(\"metadata\", {})\n",
        "        content_preview = ic[\"content\"][:200] if ic[\"content\"] else \"(base64 data)\"\n",
        "        print(f\"  Content preview: {content_preview}\")\n",
        "        if meta:\n",
        "            print(f\"  Metadata keys:   {list(meta.keys())}\")\n",
        "else:\n",
        "    print(\"\\n(No image/chart chunks detected.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Just Happened?\n",
        "\n",
        "Notice how NVIngest **separated** the document into distinct content types:\n",
        "\n",
        "- **Text chunks** contain the narrative paragraphs (executive summary, guidance, etc.)\n",
        "- **Table chunks** contain the financial data table â€” with the quarterly revenue, net income, and EPS values preserved as structured data\n",
        "- **Image chunks** contain the bar chart figure\n",
        "\n",
        "A standard text parser (e.g., `PyPDF2` or `pdfplumber`) would have flattened the table into something like:\n",
        "\n",
        "```\n",
        "Quarter Revenue ($M) Net Income ($M) EPS ($) Q1 2024 2,105 312 1.56 Q2 2024 2,498 389 1.95 ...\n",
        "```\n",
        "\n",
        "That mangled string would be nearly impossible for an LLM to parse accurately.\n",
        "NVIngest's structured extraction preserves the table's **row-column relationships**, which is what makes precise Q&A possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3 â€” Index: Build the RAG Pipeline with LlamaIndex & NVIDIA NIMs\n",
        "\n",
        "Now we'll take these structured chunks and build a proper retrieval-augmented\n",
        "generation pipeline using:\n",
        "\n",
        "- **`llama-index-llms-nvidia`** â€” connects to NVIDIA-hosted LLM (meta/llama-3.1-70b-instruct)\n",
        "- **`llama-index-embeddings-nvidia`** â€” connects to NVIDIA-hosted embedding model\n",
        "- **`llama-index-vector-stores-milvus`** â€” stores vectors in our local Milvus instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.nvidia import NVIDIA\n",
        "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
        "\n",
        "# â”€â”€ Configure NVIDIA LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "llm = NVIDIA(\n",
        "    model=\"meta/llama-3.1-70b-instruct\",\n",
        "    api_key=NVIDIA_API_KEY,\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        ")\n",
        "\n",
        "# â”€â”€ Configure NVIDIA Embedding Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "embed_model = NVIDIAEmbedding(\n",
        "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
        "    api_key=NVIDIA_API_KEY,\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        ")\n",
        "\n",
        "# â”€â”€ Set as global defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "print(f\"LLM:        {llm.model}\")\n",
        "print(f\"Embeddings: {embed_model.model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Convert NVIngest chunks into LlamaIndex Documents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#\n",
        "# We preserve the content type as metadata so the retriever can distinguish\n",
        "# between text paragraphs and structured table data.\n",
        "\n",
        "documents = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    content = chunk[\"content\"]\n",
        "    if not content or not content.strip():\n",
        "        continue\n",
        "\n",
        "    # For table chunks, prefix with a label to help the LLM identify context\n",
        "    if chunk[\"type\"] == \"table\":\n",
        "        content = f\"[TABLE DATA]\\n{content}\"\n",
        "    elif chunk[\"type\"] == \"image\":\n",
        "        content = f\"[CHART/FIGURE]\\n{content}\"\n",
        "\n",
        "    doc = Document(\n",
        "        text=content,\n",
        "        metadata={\n",
        "            \"source\": \"sample_financial_report.pdf\",\n",
        "            \"content_type\": chunk[\"type\"],\n",
        "            \"chunk_index\": i,\n",
        "        },\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"Created {len(documents)} LlamaIndex documents:\")\n",
        "for doc in documents:\n",
        "    ct = doc.metadata[\"content_type\"]\n",
        "    preview = doc.text[:80].replace(\"\\n\", \" \")\n",
        "    print(f\"  [{ct:6s}] {preview}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Build the Vector Index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#\n",
        "# Option A: Use Milvus vector store (recommended for production)\n",
        "# Option B: Use in-memory index (simpler, good for demos)\n",
        "\n",
        "USE_MILVUS = True  # Set to False for a simpler in-memory index\n",
        "\n",
        "if USE_MILVUS:\n",
        "    try:\n",
        "        from llama_index.vector_stores.milvus import MilvusVectorStore\n",
        "        from llama_index.core import StorageContext\n",
        "\n",
        "        vector_store = MilvusVectorStore(\n",
        "            uri=MILVUS_URI,\n",
        "            collection_name=\"nvidia_rag_demo\",\n",
        "            dim=1024,  # nv-embedqa-e5-v5 outputs 1024-dim vectors\n",
        "            overwrite=True,  # Fresh index each run\n",
        "        )\n",
        "\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "        index = VectorStoreIndex.from_documents(\n",
        "            documents,\n",
        "            storage_context=storage_context,\n",
        "            show_progress=True,\n",
        "        )\n",
        "        print(\"\\nâœ… Vector index built with Milvus backend.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Milvus connection failed: {e}\")\n",
        "        print(\"   Falling back to in-memory index...\")\n",
        "        USE_MILVUS = False\n",
        "\n",
        "if not USE_MILVUS:\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        show_progress=True,\n",
        "    )\n",
        "    print(\"\\nâœ… Vector index built (in-memory).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Create the Chat Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#\n",
        "# We set similarity_top_k high enough to always retrieve the table chunk\n",
        "# alongside text â€” this is critical for table-grounded Q&A.\n",
        "\n",
        "chat_engine = index.as_chat_engine(\n",
        "    chat_mode=\"condense_plus_context\",\n",
        "    similarity_top_k=10,\n",
        "    verbose=True,\n",
        "    system_prompt=(\n",
        "        \"You are a helpful financial analyst assistant. You have access to \"\n",
        "        \"Acme Corp's quarterly earnings data including revenue tables, \"\n",
        "        \"financial charts, and narrative text. When answering questions about \"\n",
        "        \"specific numbers, always cite the exact figures from the data. \"\n",
        "        \"If the data includes a table, read the specific row and column to \"\n",
        "        \"find the answer.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"âœ… Chat engine ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4 â€” The \"Aha!\" Moment: Precise Table-Grounded Q&A\n",
        "\n",
        "Now for the payoff. Let's ask questions that **require reading specific rows in a table** â€” the exact scenario where standard RAG fails.\n",
        "\n",
        "Remember the ground-truth data in our financial report:\n",
        "\n",
        "| Quarter  | Revenue ($M) | Net Income ($M) | EPS ($) |\n",
        "|----------|-------------|-----------------|--------|\n",
        "| Q1 2024  | 2,105       | 312             | 1.56   |\n",
        "| Q2 2024  | 2,498       | 389             | 1.95   |\n",
        "| Q3 2024  | 2,847       | 456             | 2.28   |\n",
        "| Q4 2024  | 3,102       | 521             | 2.61   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Query 1: The headline question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUERY 1: What was Acme Corp's revenue in Q3 2024?\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response1 = chat_engine.chat(\"What was Acme Corp's revenue in Q3 2024?\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Answer:\\n{response1.response}\")\n",
        "print(f\"\\nğŸ“ Expected: $2,847M (from the table, Q3 2024 row, Revenue column)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Query 2: Requires comparing two rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUERY 2: Compare Q2 and Q3 net income. What was the growth?\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response2 = chat_engine.chat(\n",
        "    \"Compare Acme Corp's net income between Q2 2024 and Q3 2024. \"\n",
        "    \"What was the dollar increase and percentage growth?\"\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Answer:\\n{response2.response}\")\n",
        "print(f\"\\nğŸ“ Expected: Q2=$389M â†’ Q3=$456M, increase of $67M (~17.2% growth)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Query 3: EPS â€” the most granular table lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUERY 3: What was the EPS in Q4 2024?\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response3 = chat_engine.chat(\"What was Acme Corp's earnings per share (EPS) in Q4 2024?\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Answer:\\n{response3.response}\")\n",
        "print(f\"\\nğŸ“ Expected: $2.61 (from the table, Q4 2024 row, EPS column)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Query 4: Requires understanding the full table + narrative â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUERY 4: What was the total annual revenue and what is the guidance?\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response4 = chat_engine.chat(\n",
        "    \"What was Acme Corp's total annual revenue for FY 2024, \"\n",
        "    \"and what is the company's revenue guidance for Q1 2025?\"\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Answer:\\n{response4.response}\")\n",
        "print(f\"\\nğŸ“ Expected: FY 2024 total = $10,552M; Q1 2025 guidance = $3,300-$3,500M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "### What We Demonstrated\n",
        "\n",
        "1. **NVIngest** parsed a PDF financial report and separated its content into distinct types: narrative *text*, structured *tables*, and *charts*.\n",
        "\n",
        "2. **The table was preserved as structured data** â€” not flattened into an unreadable string. This is the critical difference between NVIDIA's approach and a naive text parser.\n",
        "\n",
        "3. **LlamaIndex + NVIDIA NIMs** vectorised the structured chunks and created a chat engine capable of answering precise questions.\n",
        "\n",
        "4. **The LLM correctly answered table-specific questions** (\"What was Q3 revenue?\" â†’ \"$2,847M\") because it received the actual table structure, not garbled text.\n",
        "\n",
        "### Why This Matters for Enterprise RAG\n",
        "\n",
        "Real enterprise documents â€” financial reports, contracts, research papers â€” are **not** plain text. They contain:\n",
        "- Data tables with hundreds of rows\n",
        "- Charts and figures with axis labels\n",
        "- Multi-column layouts and footnotes\n",
        "\n",
        "Standard text extraction destroys this structure. NVIDIA's AI Data Platform preserves it, making enterprise RAG **actually work** on the documents that matter most.\n",
        "\n",
        "### Architecture Recap\n",
        "\n",
        "```\n",
        "PDF Document\n",
        "  â”‚\n",
        "  â–¼\n",
        "NVIngest (GPU-accelerated extraction)\n",
        "  â”‚  â”œâ”€â”€ text chunks\n",
        "  â”‚  â”œâ”€â”€ table chunks  â† structured rows & columns\n",
        "  â”‚  â””â”€â”€ image chunks  â† chart descriptions\n",
        "  â–¼\n",
        "NVIDIA Embedding NIM  â†’  Milvus Vector DB\n",
        "  â”‚\n",
        "  â–¼\n",
        "NVIDIA LLM NIM  â†’  Accurate, grounded answers\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try with your own enterprise PDFs (annual reports, contracts, etc.)\n",
        "- Enable the Vision Language Model for richer chart understanding\n",
        "- Explore hybrid search (dense + sparse) for even better retrieval\n",
        "- Add evaluation with RAGAS to measure answer quality\n",
        "\n",
        "---\n",
        "\n",
        "*Built with the [NVIDIA AI Blueprint for RAG](https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline) and [NVIngest](https://github.com/NVIDIA-AI-Blueprints/multimodal-pdf-data-extraction).*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
