{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA AI Data Platform: Enterprise RAG with Multimodal PDF Extraction\n",
    "\n",
    "**The Problem:** Most RAG demos work great on clean text but *fail* when they hit real enterprise documents â€” PDFs packed with data tables, charts, and mixed layouts. Ask a standard RAG system \"What was Q3 revenue?\" and you'll get a hallucinated guess, because the table got flattened into meaningless text.\n",
    "\n",
    "**The Solution:** [NVIDIA NVIngest](https://github.com/NVIDIA-AI-Blueprints/multimodal-pdf-data-extraction) (part of the NVIDIA AI Blueprint for RAG) extracts tables and charts as **separate, structured objects** â€” not just raw text. This means the LLM can actually read the table rows and give you the right number.\n",
    "\n",
    "In this notebook we will:\n",
    "1. **Ingest** a financial report PDF via the NVIngest API\n",
    "2. **Inspect** the structured JSON output â€” proving tables and charts are treated differently from text\n",
    "3. **Index** the results using LlamaIndex with NVIDIA NIMs for embeddings\n",
    "4. **Chat** with the data and get a precise, table-grounded answer\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Run these commands from the repo root before opening this notebook:\n",
    "\n",
    "```bash\n",
    "# 1. Set your NGC API key\n",
    "export NGC_API_KEY=\"your-ngc-api-key\"\n",
    "\n",
    "# 2. Log in to the NVIDIA container registry\n",
    "echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "\n",
    "# 3. Start Milvus (vector database)\n",
    "bash setup.sh    # NVIngest errors at the end are expected â€” we start it below\n",
    "\n",
    "# 4. Start NVIngest (document extraction service)\n",
    "docker compose -f docker-compose-nvingest.yaml up -d\n",
    "\n",
    "# 5. Wait for NVIngest to be ready (takes ~60s on first start)\n",
    "until curl -sf http://localhost:7670/v1/health/ready; do sleep 5; echo \"waiting...\"; done\n",
    "\n",
    "# 6. Create a Python venv and install dependencies\n",
    "python3 -m venv .venv && source .venv/bin/activate\n",
    "pip install -r requirements.txt && pip install \"setuptools<72\"\n",
    "\n",
    "# 7. Generate the sample PDF\n",
    "python generate_sample_pdf.py\n",
    "\n",
    "# 8. Launch this notebook\n",
    "jupyter lab notebooks/\n",
    "```\n",
    "\n",
    "**To tear down** when finished:\n",
    "```bash\n",
    "docker compose -f docker-compose-nvingest.yaml down -v\n",
    "bash teardown.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 â€” Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:39.808187Z",
     "iopub.status.busy": "2026-02-12T22:30:39.808075Z",
     "iopub.status.idle": "2026-02-12T22:30:40.071078Z",
     "shell.execute_reply": "2026-02-12T22:30:40.070068Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ Service endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NVIngest runtime listens on port 7670 (standalone deployment)\n",
    "NVINGEST_URL = os.getenv(\"NVINGEST_URL\", \"http://localhost:7670\")\n",
    "MILVUS_URI   = os.getenv(\"MILVUS_URI\",   \"http://localhost:19530\")\n",
    "\n",
    "# â”€â”€ NVIDIA API Key (NGC_API_KEY or NVIDIA_API_KEY) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NVIDIA_API_KEY = os.getenv(\"NGC_API_KEY\") or os.getenv(\"NVIDIA_API_KEY\", \"\")\n",
    "assert NVIDIA_API_KEY, (\n",
    "    \"Set NGC_API_KEY or NVIDIA_API_KEY in your environment.  \"\n",
    "    \"Get one at https://build.nvidia.com/\"\n",
    ")\n",
    "\n",
    "# â”€â”€ Path to the sample PDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PDF_PATH = Path(\"../data/sample_financial_report.pdf\")\n",
    "assert PDF_PATH.exists(), f\"PDF not found at {PDF_PATH}. Run: python generate_sample_pdf.py\"\n",
    "\n",
    "print(f\"NVIngest endpoint : {NVINGEST_URL}\")\n",
    "print(f\"Milvus endpoint   : {MILVUS_URI}\")\n",
    "print(f\"NVIDIA API Key    : {NVIDIA_API_KEY[:12]}...\")\n",
    "print(f\"Sample PDF        : {PDF_PATH}  ({PDF_PATH.stat().st_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:40.095329Z",
     "iopub.status.busy": "2026-02-12T22:30:40.095088Z",
     "iopub.status.idle": "2026-02-12T22:30:40.101836Z",
     "shell.execute_reply": "2026-02-12T22:30:40.101084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick health check on NVIngest\n",
    "try:\n",
    "    resp = requests.get(f\"{NVINGEST_URL}/v1/health/ready\", timeout=10)\n",
    "    print(f\"NVIngest health: {resp.status_code} â€” {resp.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not reach NVIngest at {NVINGEST_URL}: {e}\")\n",
    "    print(\"   Make sure NVIngest is running. Check: docker ps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1 â€” Ingest: Send the PDF to NVIngest\n",
    "\n",
    "NVIngest provides a REST API that accepts PDF documents and returns structured\n",
    "extractions.  Unlike a simple text parser (PyPDF2, pdfplumber), NVIngest uses\n",
    "GPU-accelerated models to:\n",
    "\n",
    "- **Detect tables** via YOLOX object detection\n",
    "- **Parse table structure** via DePlot / table-transformer\n",
    "- **OCR** any scanned content via PaddleOCR\n",
    "- **Caption charts** via a Vision-Language Model\n",
    "\n",
    "The result: each element is tagged with its *type* (`text`, `table`, `image`)\n",
    "so downstream systems can treat them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:40.103196Z",
     "iopub.status.busy": "2026-02-12T22:30:40.103080Z",
     "iopub.status.idle": "2026-02-12T22:30:45.581286Z",
     "shell.execute_reply": "2026-02-12T22:30:45.580366Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Approach A: Using the nv-ingest-client library â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# The official Python client wraps the REST API with a convenient job-based\n",
    "# interface.  If you have `nv-ingest-client` installed, this is the\n",
    "# recommended path.\n",
    "\n",
    "USE_CLIENT_LIB = True  # Set to False to use the raw REST fallback below\n",
    "\n",
    "nvingest_results = None\n",
    "\n",
    "if USE_CLIENT_LIB:\n",
    "    try:\n",
    "        from nv_ingest_client.client import NvIngestClient\n",
    "        from nv_ingest_api.util.service_clients.rest.rest_client import RestClient\n",
    "        from nv_ingest_client.primitives.tasks import (\n",
    "            ExtractTask,\n",
    "            SplitTask,\n",
    "        )\n",
    "        from nv_ingest_client.primitives.jobs import JobSpec\n",
    "        from nv_ingest_client.util.file_processing.extract import extract_file_content\n",
    "\n",
    "        print(\"âœ… nv-ingest-client library found (v25.4.2+).\")\n",
    "\n",
    "        # Create the client pointing at our local NVIngest server (port 7670)\n",
    "        client = NvIngestClient(\n",
    "            message_client_allocator=RestClient,\n",
    "            message_client_hostname=\"localhost\",\n",
    "            message_client_port=7670,\n",
    "        )\n",
    "\n",
    "        # Read and encode the PDF\n",
    "        file_content, file_type = extract_file_content(str(PDF_PATH))\n",
    "\n",
    "        # Build a job specification\n",
    "        job_spec = JobSpec(\n",
    "            payload=file_content,\n",
    "            source_id=str(PDF_PATH),\n",
    "            source_name=PDF_PATH.name,\n",
    "            document_type=file_type,\n",
    "        )\n",
    "\n",
    "        # Add extraction task â€” extract text, tables, and charts\n",
    "        extract_task = ExtractTask(\n",
    "            document_type=file_type,\n",
    "            extract_text=True,\n",
    "            extract_images=True,\n",
    "            extract_tables=True,\n",
    "        )\n",
    "        job_spec.add_task(extract_task)\n",
    "\n",
    "        # Optional: add a splitting/chunking task\n",
    "        split_task = SplitTask(\n",
    "            chunk_size=1024,\n",
    "            chunk_overlap=150,\n",
    "        )\n",
    "        job_spec.add_task(split_task)\n",
    "\n",
    "        # Submit the job\n",
    "        job_id = client.add_job(job_spec)\n",
    "        client.submit_job(job_id, job_queue_id=\"morpheus_task_queue\")\n",
    "\n",
    "        # Wait for results\n",
    "        result = client.fetch_job_result(job_id, timeout=120)\n",
    "        nvingest_results = result\n",
    "\n",
    "        print(f\"âœ… Ingestion complete. Received {len(result)} result set(s).\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  nv-ingest-client not installed. Falling back to REST API...\")\n",
    "        USE_CLIENT_LIB = False\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Client library call failed: {e}\")\n",
    "        print(\"   Falling back to REST API approach...\")\n",
    "        USE_CLIENT_LIB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:45.583083Z",
     "iopub.status.busy": "2026-02-12T22:30:45.582845Z",
     "iopub.status.idle": "2026-02-12T22:30:45.588348Z",
     "shell.execute_reply": "2026-02-12T22:30:45.587638Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Approach B: Raw REST API fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# If the nv-ingest-client is not available or the API has changed, we can\n",
    "# fall back to the NV-Ingest v2 REST API (submit_job + fetch_job).\n",
    "\n",
    "if not USE_CLIENT_LIB or nvingest_results is None:\n",
    "    print(\"Using REST API to ingest the PDF...\")\n",
    "\n",
    "    # Read and base64-encode the PDF\n",
    "    pdf_bytes = PDF_PATH.read_bytes()\n",
    "    pdf_b64 = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
    "\n",
    "    # Build a v2 submit_job payload\n",
    "    payload = {\n",
    "        \"payload\": pdf_b64,\n",
    "        \"source_id\": str(PDF_PATH),\n",
    "        \"source_name\": PDF_PATH.name,\n",
    "        \"document_type\": \"pdf\",\n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"type\": \"extract\",\n",
    "                \"task_properties\": {\n",
    "                    \"method\": \"pdfium\",\n",
    "                    \"document_type\": \"pdf\",\n",
    "                    \"extract_text\": True,\n",
    "                    \"extract_images\": True,\n",
    "                    \"extract_tables\": True,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"split\",\n",
    "                \"task_properties\": {\n",
    "                    \"chunk_size\": 1024,\n",
    "                    \"chunk_overlap\": 150,\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    # Submit the job\n",
    "    resp = requests.post(\n",
    "        f\"{NVINGEST_URL}/v2/submit_job\",\n",
    "        json=payload,\n",
    "        headers=headers,\n",
    "        timeout=300,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    submit_result = resp.json()\n",
    "    job_id = submit_result  # v2 returns the job_id directly\n",
    "    print(f\"Job submitted: {job_id}\")\n",
    "\n",
    "    # Fetch results (poll until ready)\n",
    "    import time\n",
    "    for attempt in range(60):\n",
    "        fetch_resp = requests.get(\n",
    "            f\"{NVINGEST_URL}/v2/fetch_job/{job_id}\",\n",
    "            headers=headers,\n",
    "            timeout=60,\n",
    "        )\n",
    "        if fetch_resp.status_code == 200:\n",
    "            nvingest_results = fetch_resp.json()\n",
    "            print(f\"âœ… Ingestion complete (REST v2). Status: {fetch_resp.status_code}\")\n",
    "            break\n",
    "        elif fetch_resp.status_code == 202:\n",
    "            print(f\"  Still processing... (attempt {attempt+1})\")\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            fetch_resp.raise_for_status()\n",
    "    else:\n",
    "        print(\"âš ï¸  Timed out waiting for ingestion result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 â€” Inspect: Explore the Structured Extraction Output\n",
    "\n",
    "This is the key insight: NVIngest does **not** return a flat wall of text.\n",
    "Instead, each element is classified by `type`:\n",
    "\n",
    "| Type    | What it means                                    |\n",
    "|---------|--------------------------------------------------|\n",
    "| `text`  | Narrative paragraphs, headings                   |\n",
    "| `table` | A detected table with row/column structure        |\n",
    "| `image` | A chart, figure, or infographic                  |\n",
    "\n",
    "Let's look at what came back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:45.589820Z",
     "iopub.status.busy": "2026-02-12T22:30:45.589710Z",
     "iopub.status.idle": "2026-02-12T22:30:45.597358Z",
     "shell.execute_reply": "2026-02-12T22:30:45.596749Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Parse and normalise the results into a flat list of chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# The exact shape of `nvingest_results` depends on the NVIngest version.\n",
    "# We normalise it into a list of dicts, each with at least:\n",
    "#   { \"type\": \"text\"|\"table\"|\"image\", \"content\": ..., \"metadata\": {...} }\n",
    "\n",
    "def flatten_results(raw):\n",
    "    \"\"\"Extract a flat list of content chunks from NVIngest output.\n",
    "\n",
    "    NVIngest v25+ returns results as a list of list-of-dicts.\n",
    "    Each dict has 'document_type' and 'metadata' with nested content.\n",
    "    For structured items (tables/charts), the actual text is in\n",
    "    metadata.table_metadata.table_content, NOT in metadata.content\n",
    "    (which is a base64-encoded image).\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # Handle different response shapes\n",
    "    if isinstance(raw, list):\n",
    "        items = raw\n",
    "    elif isinstance(raw, dict):\n",
    "        items = raw.get(\"data\", raw.get(\"documents\", raw.get(\"results\", [raw])))\n",
    "    else:\n",
    "        items = [raw]\n",
    "\n",
    "    for item in items:\n",
    "        if isinstance(item, list):\n",
    "            # Nested list â€” recurse\n",
    "            chunks.extend(flatten_results(item))\n",
    "        elif isinstance(item, dict):\n",
    "            meta = item.get(\"metadata\", {})\n",
    "            content_metadata = meta.get(\"content_metadata\", {})\n",
    "\n",
    "            # Determine content type\n",
    "            content_type = (\n",
    "                content_metadata.get(\"type\")\n",
    "                or item.get(\"document_type\")\n",
    "                or item.get(\"type\")\n",
    "                or \"text\"\n",
    "            )\n",
    "\n",
    "            # Extract the actual text content based on type\n",
    "            # Note: metadata fields may be None, so use `or {}` to handle that\n",
    "            table_meta = meta.get(\"table_metadata\") or {}\n",
    "            chart_meta = meta.get(\"chart_metadata\") or {}\n",
    "            description = content_metadata.get(\"description\") or \"\"\n",
    "\n",
    "            if content_type == \"structured\":\n",
    "                # For structured items (tables/charts), get table_content text\n",
    "                table_text = table_meta.get(\"table_content\") or \"\"\n",
    "                # Classify as table or chart based on description\n",
    "                if \"chart\" in description.lower() or \"figure\" in description.lower():\n",
    "                    content_type = \"image\"  # chart/figure\n",
    "                else:\n",
    "                    content_type = \"table\"\n",
    "                content = table_text if table_text else (meta.get(\"content\") or \"\")\n",
    "            elif content_type == \"text\":\n",
    "                content = (meta.get(\"content\") or \"\") or (item.get(\"content\") or \"\")\n",
    "            elif content_type == \"image\":\n",
    "                # For images, check if there's a caption or table_content\n",
    "                content = (\n",
    "                    (table_meta.get(\"table_content\") or \"\")\n",
    "                    or (chart_meta.get(\"caption\") or \"\")\n",
    "                    or description\n",
    "                    or \"Image from document\"\n",
    "                )\n",
    "                if not content or content == \"Image extracted from PDF document.\":\n",
    "                    content = \"(image data)\"\n",
    "            else:\n",
    "                content = (meta.get(\"content\") or \"\") or (item.get(\"content\") or \"\")\n",
    "\n",
    "            # Map remaining structured types\n",
    "            if content_type in (\"table_data\",):\n",
    "                content_type = \"table\"\n",
    "            if content_type in (\"chart\", \"figure\", \"infographic\"):\n",
    "                content_type = \"image\"\n",
    "\n",
    "            chunks.append({\n",
    "                \"type\": content_type,\n",
    "                \"content\": content if isinstance(content, str) else json.dumps(content, indent=2) if content else \"\",\n",
    "                \"metadata\": meta,\n",
    "            })\n",
    "        elif isinstance(item, str):\n",
    "            chunks.append({\"type\": \"text\", \"content\": item, \"metadata\": {}})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = flatten_results(nvingest_results)\n",
    "print(f\"Total chunks extracted: {len(chunks)}\")\n",
    "print()\n",
    "\n",
    "# Summary by type\n",
    "type_counts = Counter(c[\"type\"] for c in chunks)\n",
    "print(\"Chunk types:\")\n",
    "for t, count in type_counts.most_common():\n",
    "    print(f\"  {t:10s}  â†’  {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:45.598972Z",
     "iopub.status.busy": "2026-02-12T22:30:45.598863Z",
     "iopub.status.idle": "2026-02-12T22:30:45.602210Z",
     "shell.execute_reply": "2026-02-12T22:30:45.601492Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Show ALL chunks (truncated for readability) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL EXTRACTION OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    content_preview = chunk[\"content\"][:300] if chunk[\"content\"] else \"(empty)\"\n",
    "    print(f\"\\nâ”€â”€â”€ Chunk {i}  â”‚  type: {chunk['type']}  â”€â”€â”€\")\n",
    "    print(content_preview)\n",
    "    if len(chunk[\"content\"]) > 300:\n",
    "        print(f\"  ... ({len(chunk['content'])} chars total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:45.603587Z",
     "iopub.status.busy": "2026-02-12T22:30:45.603480Z",
     "iopub.status.idle": "2026-02-12T22:30:45.607163Z",
     "shell.execute_reply": "2026-02-12T22:30:45.606333Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Highlight TABLE chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "table_chunks = [c for c in chunks if c[\"type\"] == \"table\"]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸ” TABLE CHUNKS FOUND: {len(table_chunks)}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if table_chunks:\n",
    "    for i, tc in enumerate(table_chunks):\n",
    "        print(f\"\\nâ”€â”€ Table {i + 1} â”€â”€\")\n",
    "        print(tc[\"content\"][:1000])\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n(No table chunks detected. Check NVIngest configuration.)\")\n",
    "    print(\"Tip: Tables might be classified under a different type key.\")\n",
    "    print(\"Let's check all unique types:\", set(c['type'] for c in chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:45.608629Z",
     "iopub.status.busy": "2026-02-12T22:30:45.608522Z",
     "iopub.status.idle": "2026-02-12T22:30:45.612714Z",
     "shell.execute_reply": "2026-02-12T22:30:45.611954Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Highlight IMAGE / CHART chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "image_chunks = [c for c in chunks if c[\"type\"] == \"image\"]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸ–¼ï¸  IMAGE / CHART CHUNKS FOUND: {len(image_chunks)}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if image_chunks:\n",
    "    for i, ic in enumerate(image_chunks):\n",
    "        print(f\"\\nâ”€â”€ Image {i + 1} â”€â”€\")\n",
    "        # Images may be base64-encoded; show metadata instead of raw data\n",
    "        meta = ic.get(\"metadata\", {})\n",
    "        content_preview = ic[\"content\"][:200] if ic[\"content\"] else \"(base64 data)\"\n",
    "        print(f\"  Content preview: {content_preview}\")\n",
    "        if meta:\n",
    "            print(f\"  Metadata keys:   {list(meta.keys())}\")\n",
    "else:\n",
    "    print(\"\\n(No image/chart chunks detected.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Notice how NVIngest **separated** the document into distinct content types:\n",
    "\n",
    "- **Text chunks** contain the narrative paragraphs (executive summary, guidance, etc.)\n",
    "- **Table chunks** contain the financial data table â€” with the quarterly revenue, net income, and EPS values preserved as structured data\n",
    "- **Image chunks** contain the bar chart figure\n",
    "\n",
    "A standard text parser (e.g., `PyPDF2` or `pdfplumber`) would have flattened the table into something like:\n",
    "\n",
    "```\n",
    "Quarter Revenue ($M) Net Income ($M) EPS ($) Q1 2024 2,105 312 1.56 Q2 2024 2,498 389 1.95 ...\n",
    "```\n",
    "\n",
    "That mangled string would be nearly impossible for an LLM to parse accurately.\n",
    "NVIngest's structured extraction preserves the table's **row-column relationships**, which is what makes precise Q&A possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3 â€” Index: Build the RAG Pipeline with LlamaIndex & NVIDIA NIMs\n",
    "\n",
    "Now we'll take these structured chunks and build a proper retrieval-augmented\n",
    "generation pipeline using:\n",
    "\n",
    "- **`llama-index-llms-nvidia`** â€” connects to NVIDIA-hosted LLM (meta/llama-3.1-70b-instruct)\n",
    "- **`llama-index-embeddings-nvidia`** â€” connects to NVIDIA-hosted embedding model\n",
    "- **`llama-index-vector-stores-milvus`** â€” stores vectors in our local Milvus instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:45.614175Z",
     "iopub.status.busy": "2026-02-12T22:30:45.614067Z",
     "iopub.status.idle": "2026-02-12T22:30:46.451370Z",
     "shell.execute_reply": "2026-02-12T22:30:46.450434Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "\n",
    "# â”€â”€ Configure NVIDIA LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "llm = NVIDIA(\n",
    "    model=\"meta/llama-3.1-70b-instruct\",\n",
    "    api_key=NVIDIA_API_KEY,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    ")\n",
    "\n",
    "# â”€â”€ Configure NVIDIA Embedding Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "embed_model = NVIDIAEmbedding(\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "    api_key=NVIDIA_API_KEY,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    ")\n",
    "\n",
    "# â”€â”€ Set as global defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(f\"LLM:        {llm.model}\")\n",
    "print(f\"Embeddings: {embed_model.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:46.453026Z",
     "iopub.status.busy": "2026-02-12T22:30:46.452900Z",
     "iopub.status.idle": "2026-02-12T22:30:46.457568Z",
     "shell.execute_reply": "2026-02-12T22:30:46.456786Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Convert NVIngest chunks into LlamaIndex Documents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# We preserve the content type as metadata so the retriever can distinguish\n",
    "# between text paragraphs and structured table data.\n",
    "\n",
    "documents = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    content = chunk[\"content\"]\n",
    "    if not content or not content.strip():\n",
    "        continue\n",
    "\n",
    "    # Skip placeholder image chunks that have no real text content\n",
    "    if content.strip() == \"(image data)\":\n",
    "        continue\n",
    "\n",
    "    # For table chunks, prefix with a label to help the LLM identify context\n",
    "    if chunk[\"type\"] == \"table\":\n",
    "        content = f\"[TABLE DATA]\\n{content}\"\n",
    "    elif chunk[\"type\"] == \"image\":\n",
    "        content = f\"[CHART/FIGURE]\\n{content}\"\n",
    "\n",
    "    doc = Document(\n",
    "        text=content,\n",
    "        metadata={\n",
    "            \"source\": \"sample_financial_report.pdf\",\n",
    "            \"content_type\": chunk[\"type\"],\n",
    "            \"chunk_index\": i,\n",
    "        },\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"Created {len(documents)} LlamaIndex documents:\")\n",
    "for doc in documents:\n",
    "    ct = doc.metadata[\"content_type\"]\n",
    "    preview = doc.text[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"  [{ct:6s}] {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:46.459027Z",
     "iopub.status.busy": "2026-02-12T22:30:46.458919Z",
     "iopub.status.idle": "2026-02-12T22:30:51.731130Z",
     "shell.execute_reply": "2026-02-12T22:30:51.730232Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Build the Vector Index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# Option A: Use Milvus vector store (recommended for production)\n",
    "# Option B: Use in-memory index (simpler, good for demos)\n",
    "\n",
    "USE_MILVUS = True  # Set to False for a simpler in-memory index\n",
    "\n",
    "if USE_MILVUS:\n",
    "    try:\n",
    "        from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "        from llama_index.core import StorageContext\n",
    "\n",
    "        vector_store = MilvusVectorStore(\n",
    "            uri=MILVUS_URI,\n",
    "            collection_name=\"nvidia_rag_demo\",\n",
    "            dim=1024,  # nv-embedqa-e5-v5 outputs 1024-dim vectors\n",
    "            overwrite=True,  # Fresh index each run\n",
    "        )\n",
    "\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            storage_context=storage_context,\n",
    "            show_progress=True,\n",
    "        )\n",
    "        print(\"\\nâœ… Vector index built with Milvus backend.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Milvus connection failed: {e}\")\n",
    "        print(\"   Falling back to in-memory index...\")\n",
    "        USE_MILVUS = False\n",
    "\n",
    "if not USE_MILVUS:\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(\"\\nâœ… Vector index built (in-memory).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:51.732685Z",
     "iopub.status.busy": "2026-02-12T22:30:51.732571Z",
     "iopub.status.idle": "2026-02-12T22:30:51.736377Z",
     "shell.execute_reply": "2026-02-12T22:30:51.735637Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Create the Chat Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# We set similarity_top_k high enough to always retrieve the table chunk\n",
    "# alongside text â€” this is critical for table-grounded Q&A.\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    similarity_top_k=10,\n",
    "    verbose=False,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful financial analyst assistant. You have access to \"\n",
    "        \"Acme Corp's quarterly earnings data including revenue tables, \"\n",
    "        \"financial charts, and narrative text. When answering questions about \"\n",
    "        \"specific numbers, always cite the exact figures from the data. \"\n",
    "        \"If the data includes a table, read the specific row and column to \"\n",
    "        \"find the answer.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"âœ… Chat engine ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4 â€” The \"Aha!\" Moment: Precise Table-Grounded Q&A\n",
    "\n",
    "Now for the payoff. Let's ask questions that **require reading specific rows in a table** â€” the exact scenario where standard RAG fails.\n",
    "\n",
    "Remember the ground-truth data in our financial report:\n",
    "\n",
    "| Quarter  | Revenue ($M) | Net Income ($M) | EPS ($) |\n",
    "|----------|-------------|-----------------|--------|\n",
    "| Q1 2024  | 2,105       | 312             | 1.56   |\n",
    "| Q2 2024  | 2,498       | 389             | 1.95   |\n",
    "| Q3 2024  | 2,847       | 456             | 2.28   |\n",
    "| Q4 2024  | 3,102       | 521             | 2.61   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:51.737728Z",
     "iopub.status.busy": "2026-02-12T22:30:51.737619Z",
     "iopub.status.idle": "2026-02-12T22:30:52.525443Z",
     "shell.execute_reply": "2026-02-12T22:30:52.524386Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Query 1: The headline question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUERY 1: What was Acme Corp's revenue in Q3 2024?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response1 = chat_engine.chat(\"What was Acme Corp's revenue in Q3 2024?\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{response1.response}\")\n",
    "print(f\"\\nğŸ“ Expected: $2,847M (from the table, Q3 2024 row, Revenue column)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:52.527172Z",
     "iopub.status.busy": "2026-02-12T22:30:52.527014Z",
     "iopub.status.idle": "2026-02-12T22:30:56.552940Z",
     "shell.execute_reply": "2026-02-12T22:30:56.552051Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Query 2: Requires comparing two rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUERY 2: Compare Q2 and Q3 net income. What was the growth?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response2 = chat_engine.chat(\n",
    "    \"Compare Acme Corp's net income between Q2 2024 and Q3 2024. \"\n",
    "    \"What was the dollar increase and percentage growth?\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{response2.response}\")\n",
    "print(f\"\\nğŸ“ Expected: Q2=$389M â†’ Q3=$456M, increase of $67M (~17.2% growth)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:56.554587Z",
     "iopub.status.busy": "2026-02-12T22:30:56.554451Z",
     "iopub.status.idle": "2026-02-12T22:30:57.880712Z",
     "shell.execute_reply": "2026-02-12T22:30:57.879700Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Query 3: EPS â€” the most granular table lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUERY 3: What was the EPS in Q4 2024?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response3 = chat_engine.chat(\"What was Acme Corp's earnings per share (EPS) in Q4 2024?\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{response3.response}\")\n",
    "print(f\"\\nğŸ“ Expected: $2.61 (from the table, Q4 2024 row, EPS column)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-12T22:30:57.882756Z",
     "iopub.status.busy": "2026-02-12T22:30:57.882610Z",
     "iopub.status.idle": "2026-02-12T22:30:59.784420Z",
     "shell.execute_reply": "2026-02-12T22:30:59.783162Z"
    }
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Query 4: Requires understanding the full table + narrative â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUERY 4: What was the total annual revenue and what is the guidance?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response4 = chat_engine.chat(\n",
    "    \"What was Acme Corp's total annual revenue for FY 2024, \"\n",
    "    \"and what is the company's revenue guidance for Q1 2025?\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{response4.response}\")\n",
    "print(f\"\\nğŸ“ Expected: FY 2024 total = $10,552M; Q1 2025 guidance = $3,300-$3,500M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. **NVIngest** parsed a PDF financial report and separated its content into distinct types: narrative *text*, structured *tables*, and *charts*.\n",
    "\n",
    "2. **The table was preserved as structured data** â€” not flattened into an unreadable string. This is the critical difference between NVIDIA's approach and a naive text parser.\n",
    "\n",
    "3. **LlamaIndex + NVIDIA NIMs** vectorised the structured chunks and created a chat engine capable of answering precise questions.\n",
    "\n",
    "4. **The LLM correctly answered table-specific questions** (\"What was Q3 revenue?\" â†’ \"$2,847M\") because it received the actual table structure, not garbled text.\n",
    "\n",
    "### Why This Matters for Enterprise RAG\n",
    "\n",
    "Real enterprise documents â€” financial reports, contracts, research papers â€” are **not** plain text. They contain:\n",
    "- Data tables with hundreds of rows\n",
    "- Charts and figures with axis labels\n",
    "- Multi-column layouts and footnotes\n",
    "\n",
    "Standard text extraction destroys this structure. NVIDIA's AI Data Platform preserves it, making enterprise RAG **actually work** on the documents that matter most.\n",
    "\n",
    "### Architecture Recap\n",
    "\n",
    "```\n",
    "PDF Document\n",
    "  â”‚\n",
    "  â–¼\n",
    "NVIngest (GPU-accelerated extraction)\n",
    "  â”‚  â”œâ”€â”€ text chunks\n",
    "  â”‚  â”œâ”€â”€ table chunks  â† structured rows & columns\n",
    "  â”‚  â””â”€â”€ image chunks  â† chart descriptions\n",
    "  â–¼\n",
    "NVIDIA Embedding NIM  â†’  Milvus Vector DB\n",
    "  â”‚\n",
    "  â–¼\n",
    "NVIDIA LLM NIM  â†’  Accurate, grounded answers\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with your own enterprise PDFs (annual reports, contracts, etc.)\n",
    "- Enable the Vision Language Model for richer chart understanding\n",
    "- Explore hybrid search (dense + sparse) for even better retrieval\n",
    "- Add evaluation with RAGAS to measure answer quality\n",
    "\n",
    "---\n",
    "\n",
    "*Built with the [NVIDIA AI Blueprint for RAG](https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline) and [NVIngest](https://github.com/NVIDIA-AI-Blueprints/multimodal-pdf-data-extraction).*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
